{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44a51012-8f92-4d57-9b56-67fdd97b4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0978d00d-d946-4350-b3d4-597fa9cdfc0b",
   "metadata": {},
   "source": [
    "# Lesson seven: Calculating loss in the GRPO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f355b-41cd-4953-977d-396c9a657fa0",
   "metadata": {},
   "source": [
    "This lecture covers details of the GRPO loss function, including policy loss ratio, clipping, KL divergence, and shows in practice how the process uses a 'reference model' - a base model - and then a 'policy model' that's the reference model plus trainable LoRA weights. The stuff in this lesson is for learning and understanding - in practice when doing RFT we won't write and use our own implementation of the loss function... instead we'll let other code, like Predibase code, handle it, since it's always the same, plugging in things that change, like our own reward function(s). This is covered in lesson eight.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bff929-5b5e-47de-b1d4-d96d1b94bcd0",
   "metadata": {},
   "source": [
    "## Initialize the model and tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40fb7720-65fd-4e12-a04a-3112cc26ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = 'babylm/babyllama-100m-2024'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_str)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "\n",
    "# pad on left so we can add new tokens on the right\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc6a4e2f-4b14-4d57-a39c-6d34b7e6b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(16000, 512, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
      "          (down_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-06)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((512,), eps=1e-06)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=16000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0edef886-200a-4401-9946-7de8f1cfc11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1086, 1617, 2837, 6114, 4749,  551,  196,  154]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'The quick brown fox jumped over the '\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394523d3-39e3-48ff-9413-4fa46e75fe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1086, 1617, 2837, 6114, 4749,  551,  196,  154, 3407, 1952]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = base_model.generate(\n",
    "        **input_ids,\n",
    "        max_new_tokens=2,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4d1d51-a69b-4dde-94f4-3a0246bed0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over the icy ground'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48a6c20-1562-4263-8ca4-36c9a1f24010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'icy ground'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_portion = generated_text[len(prompt):]\n",
    "generated_portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ed8624-4340-417b-95ab-b19acc492dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The quick brown fox jumped over the \u001b[94micy ground\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generated text: {prompt}\\033[94m{generated_portion}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b096c1-1f3b-45f5-905f-936772cb8d30",
   "metadata": {},
   "source": [
    "## Create reference and policy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff1f6c53-ba17-4172-8cba-448c2a6e3112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(16000, 512, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (up_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (down_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((512,), eps=1e-06)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((512,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=16000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "ref_model = copy.deepcopy(base_model)\n",
    "\n",
    "# init LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # rank of update matrices\n",
    "    lora_alpha=32, # alpha scaling factor\n",
    "    target_modules=['q_proj', 'v_proj'], # apply LoRA here\n",
    "    lora_dropout=0.1,\n",
    "    init_lora_weights=False,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d885b43-2f28-4247-836d-151389e663c4",
   "metadata": {},
   "source": [
    "## Calculate policy loss ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1aa48e-57e9-43fc-968d-91f55ffe4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(prompt, completion):\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors='pt')\n",
    "    completion_tokens = tokenizer(completion, return_tensors='pt')\n",
    "\n",
    "    # combined input\n",
    "    input_ids = torch.cat(\n",
    "        [\n",
    "            prompt_tokens['input_ids'],\n",
    "            completion_tokens['input_ids'],\n",
    "        ],\n",
    "        dim=1\n",
    "    )\n",
    "    attention_mask = torch.cat(\n",
    "        [\n",
    "            prompt_tokens['attention_mask'],\n",
    "            completion_tokens['attention_mask'],\n",
    "        ],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    prompt_length = prompt_tokens['input_ids'].shape[1]\n",
    "    completion_length = completion_tokens['input_ids'].shape[1]\n",
    "    total_length = prompt_length + completion_length\n",
    "\n",
    "    # create mask to identify tokens generated by the model\n",
    "    completion_mask = torch.zeros(total_length, dtype=torch.float32)\n",
    "    completion_mask[prompt_length:] = 1.0\n",
    "\n",
    "    return input_ids, attention_mask, completion_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a64359-22d5-41eb-9d43-699a07fb53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_probs(model, input_ids, attention_mask):\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # compute the log prob for each token in the sequence\n",
    "    # output.logits is the logits for all tokens in the vocab, for each position in the sequence\n",
    "    log_probs = F.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # and extract the log prob for the actual token generated at each position in the sequence\n",
    "    return log_probs.gather(\n",
    "        dim=-1,\n",
    "        index=input_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35988ee9-5e3d-412b-b1b9-97b7bcf1f4b5",
   "metadata": {},
   "source": [
    "And now do the GRPO loss using the prepared inputs and log probs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f84095af-2a7b-4ccc-832d-5683fddabf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(model, ref_model, prompt, completion, advantage):\n",
    "    input_ids, attention_mask, completion_mask = prepare_inputs(prompt, completion)\n",
    "\n",
    "    # model forward\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask)\n",
    "    with torch.no_grad():\n",
    "        ref_token_log_probs = compute_log_probs(ref_model, input_ids, attention_mask)\n",
    "\n",
    "    # ratio = p_model / p_ref = exp(log(p_model) - log(p_ref))\n",
    "    ratio = torch.exp(token_log_probs - ref_token_log_probs)\n",
    "\n",
    "    # scale ratio by advantage func\n",
    "    policy_loss = ratio * advantage\n",
    "\n",
    "    # since we want to maximize reward, make the loss negative since optimizers minimize loss\n",
    "    per_token_loss = -policy_loss\n",
    "\n",
    "    # and only compute loss over the output tokens\n",
    "    loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0bc42a-ed6b-4e52-b367-9f18678de900",
   "metadata": {},
   "source": [
    "And call w/ a sample completion and a hard-coded advantage value (which in practice we'll calc w/ reward functions, as in prev lectures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c367b557-55cb-49f3-b8df-97ac17357053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.3921, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpo_loss(model, ref_model, prompt, 'fence and', advantage=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52b791-4c52-4264-a66d-a19a565d717e",
   "metadata": {},
   "source": [
    "The rest of the lecture shows the math to add clipping and KL divergence. I'm not going to reproduce that code here since it's truly an implementation detail and I'm not digging in to understand the math at this level right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6a1a7-aa2a-4d76-b213-8622026b859a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
