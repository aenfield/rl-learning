{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b1b3783-cebf-4909-8ab3-286535ea6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset # hugging face, I think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc749a00-ab0e-4485-97c7-bf1812096615",
   "metadata": {},
   "source": [
    "# Lesson five: Reward functions with LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d482d1-4f5a-4f93-9398-db91d1689a45",
   "metadata": {},
   "source": [
    "While most of the lectures are either totally general (the intro ones) or specific to solving Wordle (like lessons three and four), this one is specific but not to Wordle. It's interesting as a good way to think about different kinds of rewards, and especially to places where it's harder to quantify 'good' and 'bad' than something like a game or code (i.e., where things aren't verifiable directly/objectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3302986-9436-45a8-bfcd-5600ad153aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(override=True) # populate env from .env file, reload of file - 'override' - ok here\n",
    "\n",
    "MODEL_NAME='predibase/Meta-Llama-3.1-8B-Instruct-dequantized'\n",
    "\n",
    "# the examples use both OpenAI's API and Predibase's API.\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "pb_client = OpenAI(\n",
    "    base_url=os.environ['PREDIBASE_LLAMA_MODEL_URL'],\n",
    "    api_key=os.environ['PREDIBASE_API_TOKEN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea543d6-5e70-4aa8-8bb8-e4113e08c044",
   "metadata": {},
   "source": [
    "## Load earnings call data and use Predibase API to get summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6fe242-2771-4a9a-a795-b5eca3b33a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm joined by Tom Greco, our President and Chief Executive Officer; and Jeff Shepherd, our Executive Vice President and Chief Financial Officer.\n",
      "We also hope that you and your families are healthy and safe.\n",
      "The health and safety of our team members and customers has been a top priority over the past year.\n",
      "With strength across all channels, we delivered comparable store sales growth of 24.7%, and margin expansion of 478 basis points versus the prior year.\n",
      "On a two-year stack, our comp sales growth was 15.4%.\n",
      "Adjusted diluted earnings per share of $3.34 represented an all-time quarterly high for AAP, and improved more than 230% compared to Q1 2020.\n",
      "Free cash flow of $259 million was up significantly versus the prior year, and we returned over $203 million to our shareholders through a combination of share repurchases and our quarterly cash dividend.\n",
      "In addition, we recently announced an updated capital allocation framework targeting top quartile total shareholder return, highlighted by operating income growth, share repurchases and an increase in our dividend.\n",
      "This further reinforces our confidence in future cash generation and our commitment to returning excess cash to shareholders.\n",
      "As outlined in April, we are building an ownership culture, as well as a differentiated operating model at Advance.\n",
      "Over the past few years, we've made substantial investments in our brands, our digital and physical assets, and our team.\n",
      "These investments, along with external factors, enabled us to post a strong start to 2021.\n",
      "Clearly, the federal stimulus package, along with our first real winter weather in three years, was a benefit to our industry.\n",
      "From a category perspective, net sales growth was led by batteries, appearance chemicals and wipers.\n",
      "Geographically, all eight regions posted over 20% growth.\n",
      "Importantly, over the past year, the Northeast, our largest region, had been below our overall reported growth rate and well below that of our top-performing regions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('mrSoul7766/ECTSum')\n",
    "transcript = ds['train'][1]['text']\n",
    "print(transcript[:1983])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d649f-100d-4dfc-b4e2-c07ef5625b8c",
   "metadata": {},
   "source": [
    "The above is a transcript of an earnings call. Suppose for this example notebook, what we want is a good summary of the call w/ the key takeaways, and no hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454c0d97-5000-4a3b-871a-4293ab1e0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZE_PROMPT = \"\"\"Generate a concise summary of the information in the following earnings call transcript.\n",
    "\n",
    "Only respond with the summary, do not include any extraneous text.\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b2c78d-d4af-4a66-bed7-87655fa4b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advance Auto Parts reported Q1 2021 results:\n",
      "\n",
      "* Comparable store sales growth of 24.7%\n",
      "* Margin expansion of 478 basis points\n",
      "* Adjusted diluted earnings per share of $3.34, up 230% from Q1 2020\n",
      "* Free cash flow of $259 million\n",
      "* Returned $203 million to shareholders through share repurchases and dividend\n",
      "* Raised comp sales guidance to up 4-6%\n",
      "* Updated adjusted OI margin range to 9-9.2%\n",
      "* Confident in long-term strategic plans to deliver strong and sustainable total shareholder return\n"
     ]
    }
   ],
   "source": [
    "def summarize(transcript, n=1):\n",
    "    prompt = SUMMARIZE_PROMPT.format(transcript=transcript)\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "\n",
    "    return pb_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n=n,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "resp = summarize(transcript)\n",
    "summary = resp.choices[0].message.content\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278faf3-b78e-49da-b222-2e6902ff70c3",
   "metadata": {},
   "source": [
    "Interestingly, the summary I get above is much shorter (the requested 'conciseness') than the one that the lecture shows. I think I'm using the same model, but perhaps something else's changed? (Side question: if we're doing this RFT work here to 'get better summaries' and models already summarize well, especially given good prompts, and new and/or bigger models probably summarize even better, this points to wanting metrics to judge how our RFT work improves, and how it compares to existing and new models. Maybe some of the reward function work here is useful when thinking about that too, or is that 'teaching to the test' and would we want something separate?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf2c82-54f6-443b-99d2-1a2ed8a2e555",
   "metadata": {},
   "source": [
    "## Use a separate LLM as a judge of the quality of the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442b3dc-b483-4491-a72a-0998c740b9f8",
   "metadata": {},
   "source": [
    "Above we used a Predibase-hosted Llama-3.1-8B model to generate a summary. Here we'll use a separate GPT-4o-mini model, via OpenAI directly, to assign a reward score to a summary. That is, we're using the second LLM as a cheaper/faster replacement for having an SME judge and rate the quality of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ea5b10-3374-4d38-a544-c83cba570545",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT_V1 = \"\"\"\n",
    "Rate the following summary of an earnings call transcript on a \n",
    "scale from 1 to 10. \n",
    "\n",
    "1 means the summary is very poor, 10 means the summary is very good.\n",
    "\n",
    "Provide reasoning followed by the final score at the end \n",
    "surrounded by <score> tags.\n",
    "\n",
    "For example:\n",
    "\n",
    "<score>1</score>\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06afd17-5d23-4458-b9ad-cbcfa6061f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_reward_v1(\n",
    "    transcript: str,\n",
    "    summary: str,\n",
    "    model: str = 'gpt-4o-mini',\n",
    "    verbose: bool = False\n",
    ") -> float:\n",
    "    prompt = JUDGE_PROMPT_V1.format(transcript=transcript, summary=summary)\n",
    "    messages = [ { 'role': 'user', 'content': prompt } ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0, # get the most likely (and deterministic) score\n",
    "    )\n",
    "    completion = resp.choices[0].message.content\n",
    "\n",
    "    if verbose:\n",
    "        print(completion)\n",
    "\n",
    "    try:\n",
    "        match = re.search(r'<score>(\\d+)</score>', completion)\n",
    "        if match is None:\n",
    "            return 0\n",
    "\n",
    "        score = match.group(1).strip()\n",
    "        score = int(score)\n",
    "    except:\n",
    "        score = 0\n",
    "\n",
    "    return score / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336802f5-dca0-4290-9f3c-f6b5248a0010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary provided captures the key financial metrics and strategic updates from the earnings call transcript effectively. It highlights significant achievements such as the impressive comparable store sales growth, margin expansion, and the substantial increase in adjusted diluted earnings per share. Additionally, it notes the company's commitment to returning value to shareholders and updates on guidance, which are crucial for investors.\n",
      "\n",
      "However, the summary could be improved by including more context about the factors driving these results, such as the impact of federal stimulus, changes in consumer behavior, and the company's strategic initiatives. It also lacks mention of specific categories or regions that contributed to the growth, which could provide a more comprehensive view of the company's performance.\n",
      "\n",
      "Overall, while the summary is concise and covers the essential points, it misses some of the nuances and details that would give a fuller picture of the earnings call. Therefore, I would rate this summary as a 7 out of 10.\n",
      "\n",
      "<score>7</score>\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "score = judge_reward_v1(transcript, summary, verbose=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bab92-143c-4962-9337-ecf845401f90",
   "metadata": {},
   "source": [
    "Now, eight separate summaries, with a score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b093d031-f876-497d-b579-8dde9ef42228",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = summarize(transcript, n=8)\n",
    "summaries = [choice.message.content for choice in resp.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf38c48-994c-4046-b2df-ac5541c6667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.7, 0.8, 0.8, 0.8, 0.7, 0.8, 0.8]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [judge_reward_v1(transcript, summary) for summary in summaries]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae0fa8-4c1b-430a-ba08-5e4d0265f36c",
   "metadata": {},
   "source": [
    "LLMs-as-a-judge have the problem we see above where we have little diversity - no 'this is really bad' or 'this is really good', which apparently is common when using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfa067-d5ff-4ed0-876b-291580401c27",
   "metadata": {},
   "source": [
    "Finished through 5:15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab4078-1e87-4c71-b6ea-0b6aaff98c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
