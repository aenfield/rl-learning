{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b1b3783-cebf-4909-8ab3-286535ea6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from datasets import load_dataset # hugging face, I think\n",
    "\n",
    "from tabulate import tabulate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc749a00-ab0e-4485-97c7-bf1812096615",
   "metadata": {},
   "source": [
    "# Lesson five: Reward functions with LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d482d1-4f5a-4f93-9398-db91d1689a45",
   "metadata": {},
   "source": [
    "While most of the lectures are either totally general (the intro ones) or specific to solving Wordle (like lessons three and four), this one is specific but not to Wordle. It's interesting as a good way to think about different kinds of rewards, and especially to places where it's harder to quantify 'good' and 'bad' than something like a game or code (i.e., where things aren't verifiable directly/objectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3302986-9436-45a8-bfcd-5600ad153aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(override=True) # populate env from .env file, reload of file - 'override' - ok here\n",
    "\n",
    "MODEL_NAME='predibase/Meta-Llama-3.1-8B-Instruct-dequantized'\n",
    "\n",
    "# the examples use both OpenAI's API and Predibase's API.\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "pb_client = OpenAI(\n",
    "    base_url=os.environ['PREDIBASE_LLAMA_MODEL_URL'],\n",
    "    api_key=os.environ['PREDIBASE_API_TOKEN']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea543d6-5e70-4aa8-8bb8-e4113e08c044",
   "metadata": {},
   "source": [
    "## Load earnings call data and use Predibase API to get summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6fe242-2771-4a9a-a795-b5eca3b33a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm joined by Tom Greco, our President and Chief Executive Officer; and Jeff Shepherd, our Executive Vice President and Chief Financial Officer.\n",
      "We also hope that you and your families are healthy and safe.\n",
      "The health and safety of our team members and customers has been a top priority over the past year.\n",
      "With strength across all channels, we delivered comparable store sales growth of 24.7%, and margin expansion of 478 basis points versus the prior year.\n",
      "On a two-year stack, our comp sales growth was 15.4%.\n",
      "Adjusted diluted earnings per share of $3.34 represented an all-time quarterly high for AAP, and improved more than 230% compared to Q1 2020.\n",
      "Free cash flow of $259 million was up significantly versus the prior year, and we returned over $203 million to our shareholders through a combination of share repurchases and our quarterly cash dividend.\n",
      "In addition, we recently announced an updated capital allocation framework targeting top quartile total shareholder return, highlighted by operating income growth, share repurchases and an increase in our dividend.\n",
      "This further reinforces our confidence in future cash generation and our commitment to returning excess cash to shareholders.\n",
      "As outlined in April, we are building an ownership culture, as well as a differentiated operating model at Advance.\n",
      "Over the past few years, we've made substantial investments in our brands, our digital and physical assets, and our team.\n",
      "These investments, along with external factors, enabled us to post a strong start to 2021.\n",
      "Clearly, the federal stimulus package, along with our first real winter weather in three years, was a benefit to our industry.\n",
      "From a category perspective, net sales growth was led by batteries, appearance chemicals and wipers.\n",
      "Geographically, all eight regions posted over 20% growth.\n",
      "Importantly, over the past year, the Northeast, our largest region, had been below our overall reported growth rate and well below that of our top-performing regions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('mrSoul7766/ECTSum')\n",
    "transcript = ds['train'][1]['text']\n",
    "print(transcript[:1983])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d649f-100d-4dfc-b4e2-c07ef5625b8c",
   "metadata": {},
   "source": [
    "The above is a transcript of an earnings call. Suppose for this example notebook, what we want is a good summary of the call w/ the key takeaways, and no hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "454c0d97-5000-4a3b-871a-4293ab1e0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARIZE_PROMPT = \"\"\"Generate a concise summary of the information in the following earnings call transcript.\n",
    "\n",
    "Only respond with the summary, do not include any extraneous text.\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45b2c78d-d4af-4a66-bed7-87655fa4b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advance Auto Parts reported Q1 2021 results:\n",
      "\n",
      "* Comparable store sales growth of 24.7%\n",
      "* Margin expansion of 478 basis points\n",
      "* Adjusted diluted earnings per share of $3.34, up 230% from Q1 2020\n",
      "* Free cash flow of $259 million\n",
      "* Returned $203 million to shareholders through share repurchases and dividend\n",
      "* Raised comp sales guidance to up 4-6%\n",
      "* Updated adjusted OI margin range to 9-9.2%\n",
      "* Confident in long-term strategic plans to deliver strong and sustainable total shareholder return\n"
     ]
    }
   ],
   "source": [
    "def summarize(transcript, n=1):\n",
    "    prompt = SUMMARIZE_PROMPT.format(transcript=transcript)\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': prompt},\n",
    "    ]\n",
    "\n",
    "    return pb_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n=n,\n",
    "        temperature=0.9,\n",
    "    )\n",
    "\n",
    "resp = summarize(transcript)\n",
    "summary = resp.choices[0].message.content\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278faf3-b78e-49da-b222-2e6902ff70c3",
   "metadata": {},
   "source": [
    "Interestingly, the summary I get above is much shorter (the requested 'conciseness') than the one that the lecture shows. I think I'm using the same model, but perhaps something else's changed? (Side question: if we're doing this RFT work here to 'get better summaries' and models already summarize well, especially given good prompts, and new and/or bigger models probably summarize even better, this points to wanting metrics to judge how our RFT work improves, and how it compares to existing and new models. Maybe some of the reward function work here is useful when thinking about that too, or is that 'teaching to the test' and would we want something separate?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf2c82-54f6-443b-99d2-1a2ed8a2e555",
   "metadata": {},
   "source": [
    "## Use a separate LLM as a judge of the quality of the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442b3dc-b483-4491-a72a-0998c740b9f8",
   "metadata": {},
   "source": [
    "Above we used a Predibase-hosted Llama-3.1-8B model to generate a summary. Here we'll use a separate GPT-4o-mini model, via OpenAI directly, to assign a reward score to a summary. That is, we're using the second LLM as a cheaper/faster replacement for having an SME judge and rate the quality of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11ea5b10-3374-4d38-a544-c83cba570545",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT_V1 = \"\"\"\n",
    "Rate the following summary of an earnings call transcript on a \n",
    "scale from 1 to 10. \n",
    "\n",
    "1 means the summary is very poor, 10 means the summary is very good.\n",
    "\n",
    "Provide reasoning followed by the final score at the end \n",
    "surrounded by <score> tags.\n",
    "\n",
    "For example:\n",
    "\n",
    "<score>1</score>\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06afd17-5d23-4458-b9ad-cbcfa6061f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_reward_v1(\n",
    "    transcript: str,\n",
    "    summary: str,\n",
    "    model: str = 'gpt-4o-mini',\n",
    "    verbose: bool = False\n",
    ") -> float:\n",
    "    prompt = JUDGE_PROMPT_V1.format(transcript=transcript, summary=summary)\n",
    "    messages = [ { 'role': 'user', 'content': prompt } ]\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        n=1,\n",
    "        temperature=0, # get the most likely (and deterministic) score\n",
    "    )\n",
    "    completion = resp.choices[0].message.content\n",
    "\n",
    "    if verbose:\n",
    "        print(completion)\n",
    "\n",
    "    try:\n",
    "        match = re.search(r'<score>(\\d+)</score>', completion)\n",
    "        if match is None:\n",
    "            return 0\n",
    "\n",
    "        score = match.group(1).strip()\n",
    "        score = int(score)\n",
    "    except:\n",
    "        score = 0\n",
    "\n",
    "    return score / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336802f5-dca0-4290-9f3c-f6b5248a0010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summary provided captures the key financial metrics and strategic updates from the earnings call transcript effectively. It highlights significant achievements such as the impressive comparable store sales growth, margin expansion, and the substantial increase in adjusted diluted earnings per share. Additionally, it notes the company's commitment to returning value to shareholders and updates on guidance, which are crucial for investors.\n",
      "\n",
      "However, the summary could be improved by including more context about the factors driving these results, such as the impact of federal stimulus, changes in consumer behavior, and the company's strategic initiatives. It also lacks mention of specific categories or regions that contributed to the growth, which could provide a more comprehensive view of the company's performance.\n",
      "\n",
      "Overall, while the summary is concise and covers the essential points, it misses some of the nuances and details that would give a fuller picture of the earnings call. Therefore, I would rate this summary as a 7 out of 10.\n",
      "\n",
      "<score>7</score>\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "score = judge_reward_v1(transcript, summary, verbose=True)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bab92-143c-4962-9337-ecf845401f90",
   "metadata": {},
   "source": [
    "Now, eight separate summaries, with a score for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b093d031-f876-497d-b579-8dde9ef42228",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = summarize(transcript, n=8)\n",
    "summaries = [choice.message.content for choice in resp.choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf38c48-994c-4046-b2df-ac5541c6667e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.7, 0.8, 0.8, 0.8, 0.7, 0.8, 0.8]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = [judge_reward_v1(transcript, summary) for summary in summaries]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae0fa8-4c1b-430a-ba08-5e4d0265f36c",
   "metadata": {},
   "source": [
    "LLMs-as-a-judge have the problem we see above where we have little diversity - no 'this is really bad' or 'this is really good', which apparently is common when using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accfa067-d5ff-4ed0-876b-291580401c27",
   "metadata": {},
   "source": [
    "## Using LLM-generated quizzes to obtain a reward score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598428e-b603-4492-a637-ccc3e42f4f40",
   "metadata": {},
   "source": [
    "Here we'll implement a different approach to hopefully get rewards w/ more diversity. We'll give one LLM the full transcript and have it generate a multiple-choice quiz of 'important' questions using the full set of information. Then we'll have a different LLM, using only the generated summaries that we want to evaluate, take the quiz. If the summary is good, the LLM should be able to do better on the quiz.\n",
    "\n",
    "Nuts-and-bolts, we'll use structured output and pydantic to represent the generated quiz, which'll make it easier to have the other LLM 'take the quiz' (easier than something like having to parse out questions from a generated quiz in full prose/text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7472646f-fb9e-4d3c-a00a-c519bbac86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from random import shuffle\n",
    "\n",
    "QUIZ_PROMPT = \"\"\"\n",
    "Generate a multiple-choice quiz based on the information \n",
    "in the following earnings call transcript.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "1. What was the q1 adjusted earnings per share?\n",
    "a) $3.34\n",
    "b) $5.32\n",
    "c) $2.49\n",
    "d) $7.78\n",
    "\n",
    "2. By what percent did same store sales rise in q1?\n",
    "a) 29.4%\n",
    "b) 32.1%\n",
    "c) 24.7%\n",
    "d) 21.2%\n",
    "\n",
    "===== ANSWERS =====\n",
    "1. a\n",
    "2. c\n",
    "```\n",
    "\n",
    "Limit the length of the quiz to the top 10 most relevant questions for financial analysts.\n",
    "\n",
    "Transcript:\n",
    "\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee0a9d29-415e-440e-8906-0deb3826bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    text: str\n",
    "    options: list[str]\n",
    "    answer: int\n",
    "\n",
    "    def shuffle_options(self) -> None: \n",
    "        # LLMs like to pick particular answers more often - the second option it sounds like - so we'll shuffle them up\n",
    "        correct = self.options[self.answer]\n",
    "\n",
    "        shuffled = self.options.copy()\n",
    "        shuffle(shuffled)\n",
    "\n",
    "        self.options = shuffled\n",
    "        self.answer = shuffled.index(correct)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        output = [self.text]\n",
    "        for i, option in enumerate(self.options):\n",
    "            output.append(f'{chr(65+i)}. {option}')\n",
    "        return '\\n'.join(output)\n",
    "\n",
    "class Quiz(BaseModel):\n",
    "    questions: list[Question]\n",
    "\n",
    "    def shuffle_all_questions(self) -> None:\n",
    "        for question in self.questions:\n",
    "            question.shuffle_options()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        output = []\n",
    "        for i, question in enumerate(self.questions):\n",
    "            output.append(f'\\nQuestion {i}:')\n",
    "            output.append(str(question))\n",
    "        return '\\n'.join(output)\n",
    "\n",
    "def create_quiz(transcript: str):\n",
    "    prompt = QUIZ_PROMPT.format(transcript=transcript)\n",
    "    messages = [ {'role': 'user', 'content': prompt} ]\n",
    "    resp = client.beta.chat.completions.parse(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        response_format=Quiz,\n",
    "    )\n",
    "\n",
    "    quiz = resp.choices[0].message.parsed\n",
    "    quiz.shuffle_all_questions()\n",
    "\n",
    "    return quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d876fc39-938b-4a6e-9721-ea5dd1b41cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 0:\n",
      "What was the Q1 adjusted earnings per share reported by AAP?\n",
      "A. $1.00\n",
      "B. $3.34\n",
      "C. $1.50\n",
      "D. $2.49\n",
      "\n",
      "Question 1:\n",
      "By what percent did comparable store sales grow in Q1?\n",
      "A. 24.7%\n",
      "B. 32.1%\n",
      "C. 15.4%\n",
      "D. 29.4%\n",
      "\n",
      "Question 2:\n",
      "What was the increase in free cash flow compared to the prior year?\n",
      "A. $330 million\n",
      "B. $150 million\n",
      "C. $200 million\n",
      "D. $400 million\n",
      "\n",
      "Question 3:\n",
      "What was the company's adjusted operating income for Q1?\n",
      "A. $299 million\n",
      "B. $400 million\n",
      "C. $250 million\n",
      "D. $113 million\n",
      "\n",
      "Question 4:\n",
      "How much did AAP return to shareholders in Q1 through share repurchases and dividends?\n",
      "A. $300 million\n",
      "B. $250 million\n",
      "C. $150 million\n",
      "D. $203 million\n",
      "\n",
      "Question 5:\n",
      "What was the adjusted OI margin for Q1?\n",
      "A. 9%\n",
      "B. 8.5%\n",
      "C. 10%\n",
      "D. 7%\n",
      "\n",
      "Question 6:\n",
      "What is the target for the number of new stores AAP plans to open in 2021?\n",
      "A. 200 to 250\n",
      "B. 75 to 100\n",
      "C. 50 to 75\n",
      "D. 100 to 115\n",
      "\n",
      "Question 7:\n",
      "By how many basis points did the adjusted gross profit margin expand in Q1?\n",
      "A. 50 basis points\n",
      "B. 100 basis points\n",
      "C. 75 basis points\n",
      "D. 91 basis points\n",
      "\n",
      "Question 8:\n",
      "What was the expected increase in comp sales guidance for the year?\n",
      "A. 5% to 7%\n",
      "B. 4% to 6%\n",
      "C. 3% to 5%\n",
      "D. 2% to 4%\n",
      "\n",
      "Question 9:\n",
      "What was the adjusted SG&A expense in Q1?\n",
      "A. $1.5 billion\n",
      "B. $1 billion\n",
      "C. $1.2 billion\n",
      "D. $800 million\n"
     ]
    }
   ],
   "source": [
    "quiz = create_quiz(transcript)\n",
    "print(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2808530d-74f3-4c8c-bff0-98a355279a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quiz.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbfb166-9e5a-4aa1-b344-de8367c8b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question(text='What was the Q1 adjusted earnings per share reported by AAP?', options=['$1.00', '$3.34', '$1.50', '$2.49'], answer=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz.questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e09c577-f573-4164-96e2-3eb52ad1ee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the Q1 adjusted earnings per share reported by AAP?\n",
      "A. $1.00\n",
      "B. $3.34\n",
      "C. $1.50\n",
      "D. $2.49\n"
     ]
    }
   ],
   "source": [
    "print(quiz.questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05156e00-c018-490e-8c76-1f42f67301e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 3, 0, 3, 3, 1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[question.answer for question in quiz.questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81a550-af2a-4280-9cb5-182a213def3c",
   "metadata": {},
   "source": [
    "Ok, the above generates a quiz using the transcript.\n",
    "\n",
    "There's a comment in the lesson asking how we can be sure that the answers the LLM generates above for the quiz are actually the correct answers. The answer he gives is to have an LLM take the quiz using the full transcript (instead of using a summary as below) and discard/not use any of the questions where the answers above are different from what the LLM answers given the quiz and the full transcript. Not done/shown here - for this demo I just assume all of the generated answers are correct.\n",
    "\n",
    "Now, we want to have an LLM take the quiz using only the generated summary that we want to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a28ff18c-0c49-45a3-9ca2-ad2053421d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_index = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
    "index_to_letter = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "TAKE_QUIZ_PROMPT = \"\"\"Use the provided summary of a transcript \n",
    "to answer the following quiz.\n",
    "\n",
    "Quiz:\n",
    "\n",
    "{quiz}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Respond with just a list of answers and no additional text, \n",
    "for example:\n",
    "\n",
    "[A, D, C, B, B, C, D, A, A, B]\n",
    "\n",
    "You must provide an answer for all 10 questions. \n",
    "If you don't know the answer, answer with \"0\" for that question. \n",
    "Example:\n",
    "\n",
    "[A, D, 0, B, B, C, D, A, A, B]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9df935f9-047b-4042-a812-777581c48773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_quiz(summary, quiz):\n",
    "    question_strs = []\n",
    "    for question in quiz.questions:\n",
    "        question_str = question.text\n",
    "        for i, option in enumerate(question.options):\n",
    "            letter = index_to_letter[i]\n",
    "            question_str += f'\\n{letter}. {option}'\n",
    "        question_strs.append(question_str)\n",
    "    quiz_str = '\\n\\n'.join(question_strs)\n",
    "\n",
    "    prompt = TAKE_QUIZ_PROMPT.format(quiz=quiz_str, summary=summary)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[ { 'role': 'user', 'content': prompt } ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    resp_str = resp.choices[0].message.content\n",
    "\n",
    "    # convert str repr of the list to actual list of strs\n",
    "    answers = resp_str.strip('[]').split(', ')\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d23ebc07-4a02-4521-9a0a-83e967f0094c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B', 'A', '0', '0', 'B', 'A', '0', 'A', '0', '0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = take_quiz(summaries[0], quiz)\n",
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f38b5e-98a4-4256-87bd-d471a7862cdc",
   "metadata": {},
   "source": [
    "And now we can calc the proportion of correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8875ead3-d302-464a-9f4d-361bf286ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_quiz_answers(answers, quiz):\n",
    "    assert len(answers) == len(quiz.questions)\n",
    "\n",
    "    total = len(answers)\n",
    "    correct = 0\n",
    "    for answer, question in zip(answers, quiz.questions):\n",
    "        expected_answer = index_to_letter[question.answer]\n",
    "        if answer == expected_answer:\n",
    "            correct += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50851b45-36b0-4daf-9224-c543bd587033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_quiz_answers(answers, quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6635b913-86d0-4b98-9e4b-31d8b8fcdd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B', 'A', '0', '0', 'B', 'A', '0', 'A', '0', '0'],\n",
       " ['B', 'A', 'A', 'A', 'D', 'A', 'D', 'D', 'B', 'C'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(answers, [index_to_letter[question.answer] for question in quiz.questions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c668de-291c-4948-a0c8-e115e83e1715",
   "metadata": {},
   "source": [
    "And, now, using the proportion correct as the reward, we can generate the corresponding advantages, for the eight candidate summaries generated at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4574c18-6b3d-4bcf-9573-3fd1f9313d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pulled in from utils.py\n",
    "def compute_advantages(rewards: list):\n",
    "    rewards = np.array(rewards)\n",
    "    \n",
    "    # Compute the mean and standard deviation of the rewards\n",
    "    mean_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "\n",
    "    # Avoid division by zero in case of zero variance (typically happens when all rewards are 0)\n",
    "    if std_reward == 0:\n",
    "        return [0] * len(rewards)\n",
    "\n",
    "    # Divide by stddev of rewards to normalize range to 0\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86576300-badd-4929-8aec-547e9b13b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quiz_table(all_answers, rewards):\n",
    "    advantages = compute_advantages(rewards)\n",
    "    length = len(all_answers)\n",
    "    elems = list(zip(range(length), rewards, advantages))\n",
    "\n",
    "    headers = ['Index', 'Reward', 'Advantage']\n",
    "    table = tabulate(elems, headers=headers, tablefmt='grid').split('\\n')\n",
    "    for row in table:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7863491c-3336-4c42-943d-adfcf6083c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_answers = []\n",
    "quiz_rewards = []\n",
    "for summary in summaries:\n",
    "    answers = take_quiz(summary, quiz)\n",
    "    all_answers.append(answers)\n",
    "    quiz_rewards.append(score_quiz_answers(answers, quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "334fa0d4-c1cf-40d4-8fb0-20d67c46ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------+\n",
      "|   Index |   Reward |   Advantage |\n",
      "+=========+==========+=============+\n",
      "|       0 |      0.4 |   -1.13389  |\n",
      "+---------+----------+-------------+\n",
      "|       1 |      0.5 |    0.377964 |\n",
      "+---------+----------+-------------+\n",
      "|       2 |      0.5 |    0.377964 |\n",
      "+---------+----------+-------------+\n",
      "|       3 |      0.4 |   -1.13389  |\n",
      "+---------+----------+-------------+\n",
      "|       4 |      0.6 |    1.88982  |\n",
      "+---------+----------+-------------+\n",
      "|       5 |      0.5 |    0.377964 |\n",
      "+---------+----------+-------------+\n",
      "|       6 |      0.4 |   -1.13389  |\n",
      "+---------+----------+-------------+\n",
      "|       7 |      0.5 |    0.377964 |\n",
      "+---------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "print_quiz_table(all_answers, quiz_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4c419-50e3-49b0-a149-5ac5aad55ed6",
   "metadata": {},
   "source": [
    "The lecture stops here... in practice, as I'll do w/ lecture six+ for Wordle, we'd use the rewards/advantages to fine-tune a model and guide it to generate summaries that lead to more questions correctly answered. I think generally we change up the actual stuff the LLMs generate to match whatever business problem we're solving, and change up how we generate the rewards so we get good diversity, and then the rest of the training process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f95e2-a8c8-4849-bf85-8c1d5061ade2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
